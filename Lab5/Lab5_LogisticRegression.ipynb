{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Planning Methods: Part II, Spring 2021\n",
    "\n",
    "# Lab 5: Logistic Regressions\n",
    "\n",
    "**About This Lab**\n",
    "* We will be running through this notebook together. If you have a clarifying question or other question of broad interest, feel free to interrupt or use a pause to unmute and ask it! If you have a question that may result in a one-on-one breakout room (think: detailed inquiry, conceptual question, or help debugging), please ask it in the chat!\n",
    "* We recognize learning Python via Zoom comes with its challenges and that there are many modes of learning. Please go with what works best for you. That might be printing out the Jupyter notebook, duplicating it such that you can refer to the original, working directly in it. Up to you! There isn't a single right way.\n",
    "* This lab requires that you download the following files and place them in the same directory as this Jupyter notebook:\n",
    "    * `clean_property_data.csv`\n",
    "    * `properties_wtenancy.csv`\n",
    "* This data includes properties that were sold through a real estate site (like Zillow) between 2001 and 2006 in Bogota. There are apartments and houses, characteristics of the structure like area and bathrooms, and characteristics of the neighborhood like density and a proxy for neighborhood income which is called SES."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "By the end of this lab, you will have reviewed how to:\n",
    ">1. Introduce robust errors\n",
    "\n",
    "You will also learn how to:\n",
    ">1. Run a logistic regression\n",
    ">2. Analyze odds ratios\n",
    ">3. Plot predicted probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Import packages, load & clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.discrete.discrete_model import Logit\n",
    "from statsmodels.discrete.discrete_model import MNLogit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = pd.read_csv('clean_property_data.csv')\n",
    "\n",
    "# create subdataframe\n",
    "var_list = ['price_000','pop_dens','ses','house','area_m2','num_bath','pcn_green','thefts', 'year']\n",
    "data = raw[var_list].copy()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Recode variables\n",
    "Imagine that you want to see how prices change over time, while controlling for property characteristics. Controlling for property characteristics is important because housing markets change over time. For part of the year single family homes may be hot, for another it's apartments. Another year, it may be that there is a glut of small apartments and studios, so prices for those may go down. Using price/ft2 to track market changes without adjusting for property attributes, as many realtors do, is entirely inappropriate. The proposed\n",
    "approach of tracking prices over time while controlling for property\n",
    "attributes is not unusual. It is precisely how price tracking apps like\n",
    "Zillow and Trulia work. One way to track prices over time is to create dummy variables per unit of time (months or years, for example). This is what we will do next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create SES dv\n",
    "data['high_ses'] = np.where(data['ses']>=5, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create dummy variables for the year the property was sold. There are 6 unique years, and we can create a new dummy column for each year. This could be a long process, but python can make this easier for us!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the unique values in the 'year' variable\n",
    "data['year'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create dummies from categorical variable year\n",
    "dummies = pd.get_dummies(data['year'], prefix = 'yr') \n",
    "dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append the dummies to our larger dataframe\n",
    "data = pd.concat([data, dummies], axis = 1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Robust errors in multivariable regression\n",
    "One way to mitigate the impacts of heteroskedasticity is to use\n",
    "robust errors. Remember from last lab and class what heteroskedasticity is: the errors are not uniformly distributed around a line of slope zero. A consequence of heteroskedasticity is that the standard errors you estimate commonly can be unusually small and hence you may inappropriately reject or accept a null hypothesis that the coefficient is zero. Hence the use of robust errors, which isn't required but simply an option to mitigate this problem if it is present."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Without robust errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define independent variables\n",
    "ind_var = ['high_ses', 'house', 'area_m2', 'num_bath', 'pcn_green', \n",
    "           'thefts','yr_2002','yr_2003','yr_2004','yr_2005','yr_2006'] \n",
    "# note that the year variable is categorical so we need to exclude one to prevent collinearity \n",
    "# within our model - we will exclude year 2001 - we chose to have the earlier year be our base year\n",
    "\n",
    "x = data[ind_var].assign(Intercept = 1) # independent variables\n",
    "y = data['price_000'] # dependent variable\n",
    "\n",
    "model = sm.OLS(y, x).fit()\n",
    "# save the results as \"model\" - this will be useful for other functions below.\n",
    "\n",
    "model.summary2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 With robust errors\n",
    "Take a look at the output - what's changed? What's stayed the same?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.OLS(y, x).fit(cov_type='HC0') ##cov_type='HC0' introduces robust errors\n",
    "model.summary2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Logistic regression (aka logit model)\n",
    "Remember - logistic regressions are used when the dependent variables are categorical. The simplest example is when a variable can take only binary values (0 or 1). For this example, we are going to transform our price into a dummy variable, using the median as cutoff and use that as our dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Create dependent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify dummy threshold\n",
    "price_median = data['price_000'].median()\n",
    "print(price_median)\n",
    "\n",
    "# create dummy dependent variable\n",
    "data['high_price'] = np.where(data['price_000']>price_median, 1, 0)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Run logit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y = data['high_price'] # dependent variable - it's a dummy!\n",
    "x = data[ind_var].assign(Intercept = 1) # independent variables - same list as before\n",
    "\n",
    "# define and run logit model\n",
    "logit_model = Logit(y, x).fit()\n",
    "logit_model.summary2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Display odds ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# odds ratios\n",
    "or_table = np.exp(logit_model.conf_int()) # exponentiate confidence intervals\n",
    "or_table['Odds Ratio'] = np.exp(logit_model.params) # exponentiate coefficients\n",
    "\n",
    "or_table.columns = ['2.5%', '97.5%', 'Odds Ratio'] # name columns\n",
    "or_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Plot predicted probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data['num_bath'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted probabilities\n",
    "df_predict = data.copy()\n",
    "df_predict['pred_high_price'] = logit_model.predict()\n",
    "\n",
    "# plot probabilities by key independent variable\n",
    "df_predict2 = df_predict.groupby(by = 'num_bath').agg(np.mean)[['pred_high_price']]\n",
    "df_predict2.plot()\n",
    "\n",
    "# plot with labels\n",
    "plt.title('Predicted Probability of High Price Based on Number of Bathrooms')\n",
    "plt.xlabel('Number of Bathrooms')\n",
    "plt.ylabel('Probability')\n",
    "positions = (1, 2, 3, 4, 5)\n",
    "labels = ('1', '2', '3', '4', '5')\n",
    "plt.xticks(positions, labels)\n",
    "legend = ['Pr(High Price)']\n",
    "plt.legend(legend)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Multinomial logistic regression\n",
    "In this section, we're using a new version of the Bogota dataset with a tenancy variable in wich 0 = renter-occupied, 1 = owner-occupied, and 2 = vacant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data2 = pd.read_csv('properties_wtenancy.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define indpendent variables\n",
    "ind_var = ['price', 'SES', 'area', 'dist'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Run logit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y = data2['tenancy'] # dependent variable - it's a categorical variable!\n",
    "x = data2[ind_var].assign(Intercept = 1) # independent variables - new for this dataset\n",
    "\n",
    "# define and run logit model\n",
    "mnlogit_model = MNLogit(y, x).fit()\n",
    "summary = mnlogit_model.summary2()\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Display odds ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract coefficients and confidence interval values for tenancy = 0 (renter-occupied)\n",
    "df = summary.tables[1]\n",
    "conf_int = df[['[0.025','0.975]']].to_numpy()\n",
    "coef = mnlogit_model.params.iloc[:,0].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# odds ratios for tenancy = 0 (renter-occupied)\n",
    "or_table = pd.DataFrame(data = np.exp(conf_int))\n",
    "or_table['Odds Ratio'] = np.exp(coef)\n",
    "or_table.columns = ['2.5%', '97.5%', 'Odds Ratio']\n",
    "or_table.index = ['price', 'SES', 'area', 'dist', 'Intercept'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "or_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract coefficients and confidence interval values for tenancy = 1 (owner-occupied)\n",
    "df = summary.tables[2]\n",
    "conf_int = df[['[0.025','0.975]']].to_numpy()\n",
    "coef = mnlogit_model.params.iloc[:,1].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# odds ratios for tenancy = 1 (owner-occupied)\n",
    "or_table = pd.DataFrame(data = np.exp(conf_int))\n",
    "or_table['Odds Ratio'] = np.exp(coef)\n",
    "or_table.columns = ['2.5%', '97.5%', 'Odds Ratio']\n",
    "or_table.index = ['price', 'SES', 'area', 'dist', 'Intercept'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "or_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Plot predicted probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted probabilities for y = 0 (renter-occupied)\n",
    "df_predict = data2.copy()\n",
    "df_predict['pred_renter'] = mnlogit_model.predict()[:,0]\n",
    "\n",
    "# plot probabilities by key independent variable\n",
    "df_predict2 = df_predict.groupby(by = 'SES').agg(np.mean)[['pred_renter']]\n",
    "df_predict2.plot()\n",
    "\n",
    "# plot with labels\n",
    "plt.title('Predicted Probability of Renting Based on SES')\n",
    "plt.xlabel('SES')\n",
    "plt.ylabel('Probability')\n",
    "positions = (1, 2, 3, 4, 5, 6)\n",
    "labels = ('1', '2', '3', '4', '5', '6')\n",
    "plt.xticks(positions, labels)\n",
    "legend = ['Pr(renter-occupied)']\n",
    "plt.legend(legend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted probabilities for y = 1 (owner-occupied)\n",
    "df_predict = data2.copy()\n",
    "df_predict['pred_owner'] = mnlogit_model.predict()[:,1]\n",
    "\n",
    "# plot probabilities by key independent variable\n",
    "df_predict2 = df_predict.groupby(by = 'SES').agg(np.mean)[['pred_owner']]\n",
    "df_predict2.plot()\n",
    "\n",
    "# plot with labels\n",
    "plt.title('Predicted Probability of Owning Based on SES')\n",
    "plt.xlabel('SES')\n",
    "plt.ylabel('Probability')\n",
    "positions = (1, 2, 3, 4, 5, 6)\n",
    "labels = ('1', '2', '3', '4', '5', '6')\n",
    "plt.xticks(positions, labels)\n",
    "legend = ['Pr(owner-occupied)']\n",
    "plt.legend(legend)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
