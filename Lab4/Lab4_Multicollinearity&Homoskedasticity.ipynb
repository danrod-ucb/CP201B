{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Planning Methods: Part II, Spring 2021\n",
    "\n",
    "# Lab 4: Multicollinearity & Homoskedasticity\n",
    "\n",
    "**About This Lab**\n",
    "\n",
    "*DISCLAIMER: WE HAVE NOT COVERED THIS TOPIC IN CLASS. OUR NEXT TWO CLASSES WILL COVER THIS. THE EMPHASIS OF THIS LAB IS ON THE MECHANICS OF DETECTING THESE PROBLEMS.\n",
    "\n",
    "* We will be running through this notebook together. If you have a clarifying question or other question of broad interest, feel free to interrupt or use a pause to unmute and ask it! If you have a question that may result in a one-on-one breakout room (think: detailed inquiry, conceptual question, or help debugging), please ask it in the chat!\n",
    "* We recognize learning Python via Zoom comes with its challenges and that there are many modes of learning. Please go with what works best for you. That might be printing out the Jupyter notebook, duplicating it such that you can refer to the original, working directly in it. Up to you! There isn't a single right way.\n",
    "* This lab requires that you download the following file and place it in the same directory as this Jupyter notebook:\n",
    "    * `clean_property_data.csv`\n",
    "* This data includes properties that were sold through a real estate site (like Zillow) between 2001 and 2006 in Bogota. There are apartments and houses, characteristics of the structure like area and bathrooms, and characteristics of the neighborhood like density and a proxy for neighborhood income which is called ses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "By the end of this lab, you will have reviewed how to:\n",
    ">1. Call a correlation matrix\n",
    ">2. Run multivariable linear regression\n",
    "\n",
    "You will also learn how to:\n",
    ">1. Use a and interpret the Variance Inflation Factor (VIF)\n",
    ">2. Calculate predicted values using regression model \n",
    ">3. Detect heteroskedasticity using plots \n",
    ">4. Detect heteroskedasticity using the Breusch-Pagan test\n",
    ">5. Use robust errors if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import packages and  read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.stats.api as sms\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import pearsonr\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor \n",
    "\n",
    "from statsmodels.compat import lzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in your data \n",
    "data =\n",
    "\n",
    "#Explore your data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a subdataframe, create a binary variable called 'high_ses', and explore your dataframe\n",
    "df = \n",
    "df['high_ses'] = \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Estimate a multivariable linear regression\n",
    "\n",
    "We will first run a multivariable linear regression similar to the one we used in Lab 3. In the later sections, we will evaluate our results by checking for multicollinearity and heteroskedasticity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a list of independent variables of interest \n",
    "ind_var = \n",
    "\n",
    "#Define our explanatory and dependent variables \n",
    "x = \n",
    "y = \n",
    "\n",
    "#Let's save the results of the regression as model.\n",
    "#This is be helpful for functions we will use below \n",
    "model = \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Multicollinearity \n",
    "\n",
    "*THIS HAPPENS WHEN ONE OR MORE INDEPENDENT VARIABLES ARE CLOSELY RELATED WITH EACH OTHER. AS A RESULT, THE VARIABLE(S) DO NOT ADD INFORMATION TO YOUR ABILITY TO MAKE A BETTER GUESS ABOUT THE DEPENDENT VARIABLE. AS A RESULT, THE ESTIMATED COEFFICIENT HAS A LOT OF ERROR (THAT IS, THE T-STATISTIC IS LOW, THE P-VALUE IS HIGH, AND THE 95% CONFIDENCE INTERVAL INCLUDES ZERO).\n",
    "\n",
    "*YOU RUN THIS TEST AS A WAY TO MAKE SURE THAT MULTICOLLINEARITY IS NOT RENDERING ONE OR MORE OF YOUR COEFFICIENTS AS INSIGNIFICANT.\n",
    "\n",
    "*IF MULTICOLINEARITY IS HIGH, THEN YOU KNOW WHY YOUR COEFFICIENT IS NOT SIGNIFICANT. IF IT IS LOW, IT MUST BE SOMETHING ELSE...\n",
    "\n",
    "#### 3.1 Correlation Matrix\n",
    "\n",
    "We will first use a correlation matrix to examine multicollinearity. We would like to know if any of our explanatory variables are highly correlated to one another as this might affect our regression. Let's do this using `ind_var`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call up your correlation matrix \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Variance Inflation Factor (VIF)\n",
    "\n",
    "The VIF is a better way of detecting multicollinearity because it checks whether combinations of independent variables, in addition to each one, help explan the other independent variables. By contrast, the correlation matrix above only shows us pairwise correlations. It is possible that two variables (for example, house and # of baths, more closely predicts the area of a property than house or # of baths independently.  \n",
    "\n",
    "How does the VIF work? By regressing each independent variable on all others. Python (and stata and R) does this automatically.  For the model above, it runs 7 regressions (there are 7 independent variables).  In each regression, a different independent variable becomes DEPENDENT, while all others remain independent. For example:\n",
    "\n",
    "Reg 1: Dependent: high_ses; independent: All other independent variables\n",
    "Reg 2: Dependent: house; indepdendent: All other independent variables\n",
    ":\n",
    ":\n",
    "Reg 7: Dependent: pop_dens; indepdendent: All other independent variables\n",
    "\n",
    "\n",
    "If the R2 in each regression is high, it means that all other independent variables explain the new dependent variable very well, so the new dependent variable does not bring much new information to the original regression model. That variable would have a high VIF.\n",
    "\n",
    "Formally, the VIF for each variable i is:\n",
    "VIF = 1/(1-R2), where R2 is the model where i is the dependent variable.\n",
    "\n",
    "A VIF of 5 or greater can be problematic, suggesting that high colinearity may be the reason for the lack of significance in your coefficient. (Note: For a given independent variable, a VIF of 5 means that the R2 of the regression equation with that variable as dependent and all other as independent is 0.8 or 80%. That is, 80% of the variation in that variable is explained by all other independent variables. In other words, only 20% of the variation in the variable is unique to it, and hence its possible limited explanatory power. A higher VIF (e.g., 10) means a higher R2 for the regression witth that variable as dependent and all other as independent (e.g., R2 of 0.9 or 90%).\n",
    "\n",
    "In terms of the commands below, it is important to note that VIF does not detect dataframes, therefore we extract the values of x using `df.values` into an array. You could also use `np.array` to do this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's verify the type of x in python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will now obtain an array of our x values \n",
    "x_array = \n",
    "\n",
    "#Let's verify the type of x_array\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will now obtain a VIF for each of the coefficients in our model \n",
    "vif = [variance_inflation_factor(x_array, i) for i in range(x.shape[1])]\n",
    "vif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! What are these numbers?! What is going on? Let's add some labels to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's label the VIF values that we obtained \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also obtain the VIF for a particular explanatory variable. Let's explore these steps a little "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will calculate the shorthand VIF for 'high_ses'\n",
    "#Create a sub-dataframe of your independent variables \n",
    "sub_ind = \n",
    "\n",
    "#Let's define our x and our y \n",
    "x = \n",
    "y = \n",
    "\n",
    "#Run your regression \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's now calculate the VIF using the formula VIF = 1/(1 - R^2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Predictions \n",
    "Remember that one way to determine how well our model fit is comparing our guesses (the prediction) with actual values. Here we calculate make this comparison by calculating the difference between observed and predicted for each row of data. These are called residuals. The purpose in this case, however, isn't to determine model fit but to make sure that the model is not violating a critical assumption: That residuals are not uniformly distributed \n",
    "\n",
    "Earlier we estimated a linear regression which calculates the association between our y (price), and our explanatory variables. The coefficients we obtain can help us draw a regression line for price in this way: \n",
    "\n",
    "y = 11441 + (70994 * high_ses) + (-27823 * house) + (571 * area_m2) +(10065 * num_bath) +  (226 * pcn_green) + (-16 * thefts) + (-12 * pop_dens)\n",
    "\n",
    "Keep in mind that we saved the results of the linear regression under the \"model\" variable above:  \n",
    "> model = sm.OLS(y, x).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's re-run the regression we obtained in part 2\n",
    "x = df[ind_var].assign(Intercept = 1)\n",
    "y = df['price_000']\n",
    "\n",
    "model = sm.OLS(y, x).fit()\n",
    "model.summary2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To simply look at the coefffients we obtained we can use the following:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now try to make a  prediction. Today, we will use the first line of observed data from our dataframe as seen below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's call our first line of x observations \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our prediction will look like this:\n",
    "\n",
    "y = 11441 + (70994 * 0) + (-27823 * 0) + (571 * 70) +(10065 * 2) +  (226 * 1.74)                   + (-16 * 39.92) + (-12 * 830.78)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can ask python to find the predicted value of y using this first line\n",
    "#of our observed values\n",
    "#We can do this using our dataframe x:\n",
    "\n",
    "#Or we can use our x_array:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How does this value compare to the observed value of price in our dataset\n",
    "#Let's call up the first line of the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's now calculate the residual of our first prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can also find the predictions and residuals for the entirety of our dataset\n",
    "#Let's see what our predictions are\n",
    "\n",
    "#Let's see what our residuals are\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Homoskedasticity\n",
    "\n",
    "#### 4.1 Scedasticity Plots\n",
    "\n",
    "Using the information we have gathered on our predicted values, and residuals, we can do a visual and a statistical test for homoskedasticity.\n",
    "\n",
    "The visual test graphs the predicted values (also called fitted values) in the X-axis and the residuals in the Y-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def homosked_plots(prediction, residuals):\n",
    "    #Define the figure\n",
    "    \n",
    "    #Define x, and y \n",
    "    \n",
    "    #Create title for your figure\n",
    "    \n",
    "    #Create your plot, and set your labels\n",
    "    \n",
    "    #Formats axis number to include thousands separator\n",
    "    ax.get_xaxis().set_major_formatter(plt.FuncFormatter(lambda x1, loc: \"{:,}\".format(int(x1))))\n",
    "    ax.get_yaxis().set_major_formatter(plt.FuncFormatter(lambda y1, loc: \"{:,}\".format(int(y1))))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "homosked_plots(pred_mod, res_mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Breusch-Pagan Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's create a list of tests \n",
    "test_names = \n",
    "\n",
    "#Call the Breusch-Pagan test in python\n",
    "results = sms.het_breuschpagan(model.resid, model.model.exog)\n",
    "\n",
    "#Concatenate the test values to their names using lzip\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Robust Errors: Correcting for heteroscedasticity, if present \n",
    "The following is one approach to \"correcting\" for heteroskedasticity. Other approaches include: adding more variables, transforming the dependent variable (for example, with a log-transformation), or transforming independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run your regression and introduce robust errors within your code \n",
    "#cov_type='HC0' introduces robust errors\n",
    "model = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
